{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python381064bitvenvvenvc5206c77ee25436a9af873d7e9575275",
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Metrics and datasets for this task"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Extractive summarization mainly measured by several word- and n-gram-similarity scoring algorithms, which calculate the fraction of target text that was produced by the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "From [StackExchange](https://datascience.stackexchange.com/a/75642/122618):\n",
    "\n",
    "*The problem with have an automatic evaluation system for a text summarisation model is that, although we can assess fluency from a language model, we can't really assess whether the model has pulled \"the most salient\" pieces of information from the original, longer text (and this can subjective from person to person). Hence why we need multiple human reference summaries to compute ROUGE and BLEU. However, as you are aware, these metrics have their limitations.*\n",
    "\n",
    "*ROUGE is essentially a further development of BLEU, which have been commonly used a dubious proxy for output text fluency in research to compare summarisation and translation models. These metrics are dubious because they simply look at how much they overlap with reference texts from humans (https://rxnlp.com/how-rouge-works-for-evaluation-of-summarization-tasks/#.Xt1ewy-ZOi4).*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Common metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### BLEU\n",
    "BLEU works by computing the precision — the fraction of tokens from the candidate that appear, or are “covered”, by the references— but with a twist. Like any precision-based metric, the value of the BLEU score is always a number between 0 (worst) and 1 (best).  It also penalizes words that appear in the candidate more times than it appears in any of the references.\n",
    "\n",
    "([src](https://towardsdatascience.com/nlp-metrics-made-simple-the-bleu-score-b06b14fbdbc1))"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Rouge\n",
    "\n",
    "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation\n",
    "\n",
    " It is essentially of a set of metrics for\n",
    "evaluating automatic summarization of texts as well as machine translation. It works by comparing an\n",
    "automatically produced summary or translation against a set of reference summaries (typically human-\n",
    "produced).Let us say, we have the following system and reference summaries\n",
    "\n",
    "ROUGE-N, ROUGE-S and ROUGE-L can be thought of as the granularity of texts being compared between\n",
    "the system summaries and reference summaries. For example, ROUGE-1 refers to overlap of unigrams\n",
    "between the system summary and reference summary. ROUGE-2 refers to the overlap of bigrams between\n",
    "the system and reference summaries\n",
    "\n",
    "([src](http://www.ccs.neu.edu/home/vip/teach/DMcourse/5_topicmodel_summ/notes_slides/What-is-ROUGE.pdf))\n",
    "\n",
    "Python implementation:\n",
    "https://github.com/IlyaGusev/rouge"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### METEOR\n",
    "\n",
    "The Meteor automatic evaluation metric scores machine translation hypotheses by aligning them to one or more reference translations. Alignments are based on exact, stem, synonym, and paraphrase matches between words and phrases. Segment and system level metric scores are calculated based on the alignments between hypothesis-reference pairs. \n",
    "\n",
    "([src](https://www.cs.cmu.edu/~alavie/METEOR/))"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}