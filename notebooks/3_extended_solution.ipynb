{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Papers that somewhat resemble my ideas or just may be useful*:  \n",
    "+ [Neural Extractive Text Summarization with Syntactic Compression](https://aclanthology.org/D19-1324.pdf) *by Jiacheng Xu and Greg Durrett*:\n",
    "    + The approach encodes the text and *then* performs the compression;\n",
    "    + https://github.com/jiacheng-xu/neu-compression-sum (gosh, what a horrible code)\n",
    "+ [Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting](https://aclanthology.org/P18-1063.pdf) *by Yen-Chun Chen and Mohit Bansal*:\n",
    "    + This approach utilizes RL to rewrite sents, along with abstractive summarization;\n",
    "    + https://github.com/ChenRocks/fast_abs_rl (much more readable repo)\n",
    "+ [Extractive Summarization as Text Matching](https://arxiv.org/pdf/2004.08795v1.pdf) *by Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang, Xipeng Qiu, Xuanjing Huang*:\n",
    "    + https://github.com/maszhongming/MatchSum\n",
    "    + CNN/DailyMail to a new level (44.41 in ROUGE-1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work structure\n",
    "#### 1. Dataset loading and preprocessing\n",
    "\n",
    "#### 2. Implementing heuristics as standalone preprocessing functionality (postponed)\n",
    "My idea is to use some preprocessing tricks to improve the actual effect of summarization. \n",
    "The following ways are suggested (each is to be implemented in separate notebook):\n",
    "1. Utilize coreference resolution among sentences so we won't miss important nouns in our summary.\n",
    "2. Try to split compound sentences into few smaller ones.\n",
    "3. Compress resulting sentences so we exclude some low-informative words without (hopefully) sacrifising the readability.\n",
    "    - Named entities intuitively seem more important that common nouns, so they are not to be deleted.\n",
    "    \n",
    "#### 3. Implementation of summarization block per-se\n",
    "#### 4. Evaluation and metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here: https://cs.nyu.edu/~kcho/DMQA/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Heuristics (postponed)\n",
    "\n",
    "See notebooks 3.1, 3.2, 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summarization block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my summarization block I'd like to make use of the novel method suggested by Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang, Xipeng Qiuâ€ , Xuanjing Huang in the paper [Extractive Summarization as Text Matching](https://arxiv.org/pdf/2004.08795.pdf)\n",
    "\n",
    "The main keypoint of the work:\n",
    "> Instead of scoring and extracting sentences\n",
    "> one by one to form a summary, we formulate\n",
    "> extractive summarization as a semantic text matching\n",
    "> problem and propose a novel summary-level\n",
    "> framework. Our approach bypasses the difficulty\n",
    "> of summary-level optimization by contrastive learning,\n",
    "> that is, a good summary should be more\n",
    "> semantically similar to the source document than the\n",
    "> unqualified summaries.\n",
    "\n",
    "In fact they fine-tune some bert to produce embeddings in a way they favor semantic similarity between a gold summary and a text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the loss...\n",
    "> In order to fine-tune Siamese-BERT, we use a margin-based triplet loss to update the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't search though all possible candidates can be of $\\sum_{i=1}^{n}C_n^i$ variants?\n",
    "> In the inference phase, we formulate extractive summarization as a task to search for the best summary among all the candidates C extracted from the document D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and yeah, exactly:\n",
    "> The matching idea is more intuitive while it suffers from combinatorial explosion problems. \\[...] we introduce a content selection module to pre-select salient sentences.\n",
    "\n",
    "Abovementioned content selection is done via [PreSumm](https://github.com/nlpyang/PreSumm) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Suggested flow\n",
    "The paper suggests the following workflow:\n",
    "1. You score the sentences of the input text with some third-party model accordingly to their presumed informational contribution to the meaning of a text.\n",
    "2. You get some summary candidates based on combinatorial allocations (hyperparam-dependednt, so you can affect the number of output sentences) with respect to the top scores from the step 1, yet the training itself depends on this choise so I assume if one wants to get good summaries with an arbitary number of sentences, they shall better train the model on 1..n combinations.\n",
    "3. The deeplearning model learns to choose the best one from the summaries, at the same time avoiding common pitfalls of usual models (the authors of the paper suggest \"pearl-summary vs. best-summary\" problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\[Preparation] Creating dataset\n",
    "\n",
    "The dataset I will use needs to be in a certain form. The original solution suggests jsonl format with json objects separated by newline token.\n",
    "I don't mind it. \n",
    "\n",
    "So, first of all, I preprocess and convert my dataset into suitable format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for that can be found in `dataset_utils/cnndm_preprocessor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also I truncate the dataset to 10k first docs for training and 2k docs for validation. The original paper states that the training took 30 hours with several top GPUs with all the several hundred thousand docs. I have none of such compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are suggested to somehow score our sentences by their importance. I will use trivial method for that, and also I will not truncate my input on this stage, as long as it can be a hyperparam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " > - truncate each document into the 5 most important sentences (using BertExt), \n",
    "   then select any 2 or 3 sentences to form a candidate summary, so there are C(5,2)+C(5,3)=20 candidate summaries.\n",
    "   if you want to process other datasets, you may need to adjust these numbers according to specific situation.\n",
    "\n",
    "BertExt has very questionable codebase and maintainment, so I will stuck with centroidal sorting for this case.\n",
    "Moreover, I will use not Bert or RoBERTa, but LaBSE simply because I've already used.\n",
    "Thus, if my integrated encoder will be LaBSE, why shouldn't I just utilize it to create ranking of the sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for this step can be found in  `dataset_utils/create_sentence_ranking`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loader\n",
    "The dataset loader was recreated inspired by the MatchSum repo, but with much more readable variable names and with use of torch Dataset instead of any relation on fastNLP library.\n",
    "\n",
    "It performs tokenization on the go, as well as provides candidate summary combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATASET LOADERS\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset_utils.dataset import CNNDMDataset\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_train_path = Path(\"../data/cnndm/dataset10k.jsonl\")\n",
    "indices_train_path = Path(\"../data/cnndm/sent_id10k.jsonl\")\n",
    "dataset_eval_path = Path(\"../data/cnndm/dataset2k.jsonl\")\n",
    "indices_eval_path = Path(\"../data/cnndm/sent_id2k.jsonl\")\n",
    "\n",
    "SUMMARY_LENGTH = 5\n",
    "train_dataset = CNNDMDataset(dataset_train_path, indices_train_path, SUMMARY_LENGTH=SUMMARY_LENGTH)\n",
    "eval_dataset = CNNDMDataset(dataset_eval_path, indices_eval_path, SUMMARY_LENGTH=SUMMARY_LENGTH)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=False, batch_size=8)\n",
    "eval_dataloader = DataLoader(eval_dataset, shuffle=False, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "class MatchSum(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MatchSum, self).__init__()\n",
    "\n",
    "        self.encoder = AutoModel.from_pretrained(\"cointegrated/LaBSE-en-ru\")\n",
    "        self.hidden_size = 768\n",
    "\n",
    "    def forward(self, \n",
    "        tokenized_text_sentences, \n",
    "        tokenized_candidate_sentences, \n",
    "        tokenized_summary_sentences):\n",
    "        \n",
    "        batch_size = tokenized_text_sentences.size(0)\n",
    "        # candidate_num = \n",
    "\n",
    "        # Get document embedding [SHALL CONSIDER BATCH]\n",
    "        text_out = self.encoder(**tokenized_text_sentences).pooler_output\n",
    "        text_embedding = torch.mean(text_out, dim=1)\n",
    "        text_embedding = nn.functional.normalize(text_embedding)\n",
    "\n",
    "        assert text_embedding.size() == (batch_size, self.hidden_size) # [batch_size, hidden_size]\n",
    "        \n",
    "        # Get summary embedding\n",
    "        summary_out = self.encoder(**tokenized_summary_sentences).pooler_output\n",
    "        summary_embedding = torch.mean(summary_out, dim=1)\n",
    "        summary_embedding = nn.functional.normalize(summary_embedding)\n",
    "\n",
    "        assert summary_embedding.size() == (batch_size, self.hidden_size) # [batch_size, hidden_size]\n",
    "\n",
    "        # Get candidates embedding \n",
    "        candidates_out = self.encoder(**tokenized_candidate_sentences).pooler_output\n",
    "        candidate_embedding = torch.mean(candidate_out, dim=1)\n",
    "        candidate_embedding = nn.functional.normalize(candidate_embedding)\n",
    "        # [batch_size, candidate_num, hidden_size]\n",
    "        \n",
    "        # get summary score\n",
    "        summary_score = torch.dot(summary_embedding, text_embedding) # similar to cosine cuz normalized\n",
    "\n",
    "        # get candidate score        \n",
    "        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        candidate_scores = cos(candidate_embedding, text_embedding)\n",
    "\n",
    "        \n",
    "        assert candidate_scores.size() == (batch_size, candidate_num)\n",
    "\n",
    "        return {\"candidate_scores\": candidate_scores, \"summary_score\": summary_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the paper, the loss shall be the following:\n",
    "+ We calculate pairwise losses between the gold summary and the candidate summaries\n",
    "$$\n",
    "    L_1 = max(0,f(D,C) âˆ’ f(D,C^âˆ—) + Î³_1), \n",
    "$$\n",
    "where\n",
    "+ $D$ - document\n",
    "+ $C$ - candidate summary\n",
    "+ $C^*$ - gold summary\n",
    "+ $Î³_1$ - margin value\n",
    "\n",
    "In addition, we calculate pairwise candidate losses, keeping in mind that they're already sorted by ROUGE with respect to the gold summary:\n",
    "$$\n",
    "L2 = max(0, f(D,C_j) âˆ’ f(D,C_i) + (jâˆ’i) âˆ— Î³_2)\n",
    "$$\n",
    "- where $i<j$\n",
    "- $i$ - summary rank\n",
    "The total loss is the sum of the two above.\n",
    "\n",
    "> We choose $Î³_1 = 0$ and $Î³_2 = 0.01$. When $Î³_1<0.05$ and $0.005<Î³_2<0.05$ they have little effect on performance, otherwise they will cause performance degradation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOSS FUNCTION\n",
    "MARGIN = 0.01\n",
    "def loss_function(candidate_scores, summary_score, MARGIN=MARGIN):\n",
    "    ones = torch.ones(score.size()).cuda(score.device)\n",
    "    margin_loss = torch.nn.MarginRankingLoss(0.0)\n",
    "    total_loss = margin_loss(candidate_scores, candidate_scores, ones)\n",
    "    \n",
    "    # candidate loss\n",
    "    n_candidates = candidate_scores.size(1)\n",
    "    for i in range(1, n_candidates):\n",
    "        pos_score = candidate_scores[:, :-i]\n",
    "        neg_score = candidate_scores[:, i:]\n",
    "        pos_score = pos_score.contiguous().view(-1)\n",
    "        neg_score = neg_score.contiguous().view(-1)\n",
    "        \n",
    "        ones = torch.ones(pos_score.size()).cuda(candidate_scores.device)\n",
    "        \n",
    "        margin_loss = torch.nn.MarginRankingLoss(MARGIN * i)\n",
    "        total_loss += margin_loss(pos_score, neg_score, ones)\n",
    "\n",
    "    # gold summary loss\n",
    "    pos_score = summary_score.unsqueeze(-1).expand_as(candidate_scores)\n",
    "    neg_score = candidate_scores\n",
    "    pos_score = pos_score.contiguous().view(-1)\n",
    "    neg_score = neg_score.contiguous().view(-1)\n",
    "    ones = torch.ones(pos_score.size()).cuda(candidate_scores.device)\n",
    "    margin_loss = torch.nn.MarginRankingLoss(0.0)\n",
    "    total_loss += margin_loss(pos_score, neg_score, ones)\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate schedule is the following, including the use of warm-up phase:\n",
    "$$\n",
    " lr = 2e^{âˆ’3} Â·min(step^{âˆ’0.5},step Â· wm^{âˆ’1.5})\n",
    "$$\n",
    "+ where each $step$ is a batch size of 32 \n",
    "+ and $wm$ denotes warmup steps of 10,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LR SCHEDULER\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class LRScheduler(_LRScheduler):\n",
    "    def __init__(self, optimizer, min_lr=e-6, update_every=2, max_lr=2e-5, warmup_steps=10000):\n",
    "        self.optimizer = optimizer\n",
    "        self.update_every = update_every\n",
    "        self.max_lr = max_lr\n",
    "        self.min_lr = min_lr  # min learning rate > 0 \n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.last_epoch = last_epoch\n",
    "        super(LambdaLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        # warm up\n",
    "        if self.step % self.update_every == 0 and self.step > 0:\n",
    "            self.real_step = self.step // self.update_every \n",
    "            cur_lr = self.max_lr * 100 * min(self.real_step**(-0.5), self.real_step * self.warmup_steps**(-1.5))\n",
    "            cur_lr = max(cur_lr, self.min_lr)\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = cur_lr\n",
    "            \n",
    "            return [cur_lr]\n",
    "\n",
    "#     def on_step_end(self):\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPTIMIZER\n",
    "from torch.optim import Adam\n",
    "model = MatchSum()\n",
    "optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0)\n",
    "lr_scheduler = LRScheduler(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAINING LOOP\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "from tqdm import tqdm\n",
    "NUM_TRAINING_STEPS = 10000\n",
    "NUM_EPOCHS = 5\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch in tqdm(train_dataloader, total=NUM_TRAINING_STEPS):\n",
    "        batch = {k, v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = loss_function(**outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(\"./trained_model\")\n",
    "torch.save(model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Eval and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MatchSum()\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python381064bitvenvvenvc5206c77ee25436a9af873d7e9575275"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
